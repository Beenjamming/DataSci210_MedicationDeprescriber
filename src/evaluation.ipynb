{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\krisr\\DataSci210_MedicationDeprescriber\\src\\extraction.py:17: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"NeuML/pubmedbert-base-embeddings\")\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\krisr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\krisr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\krisr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "# from langchain_groq import ChatGroq\n",
    "# from ragas import evaluate\n",
    "# from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "# from ragas.llms import LangchainLLMWrapper\n",
    "# from ragas.metrics import answer_relevancy, faithfulness, context_recall, context_precision\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "# from datasets import Dataset\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_recall_fscore_support\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "import pandas as pd\n",
    "# import requests\n",
    "import time\n",
    "from pathlib import Path\n",
    "from main import main\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_multiclass_classification(y_true, y_pred, class_labels):\n",
    "    \"\"\"\n",
    "    Evaluates a multiclass classification model.\n",
    "    \n",
    "    y_true: Ground truth labels\n",
    "    y_pred: Predicted labels from the classifier\n",
    "    class_labels: List of class names\n",
    "    \n",
    "    Returns a dictionary of accuracy, precision, recall, F1, and confusion matrix.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Accuracy\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    # Precision, Recall, F1-score\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='macro')  # macro-averaging\n",
    "    \n",
    "    # Classification report (optional detailed breakdown for each class)\n",
    "    class_report = classification_report(y_true, y_pred, target_names=class_labels)\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # Print evaluation metrics\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision (Macro Avg): {precision:.4f}\")\n",
    "    print(f\"Recall (Macro Avg): {recall:.4f}\")\n",
    "    print(f\"F1 Score (Macro Avg): {f1:.4f}\")\n",
    "    print(\"\\nClassification Report:\\n\", class_report)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "    \n",
    "    # Return metrics for further analysis if needed\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision_macro\": precision,\n",
    "        \"recall_macro\": recall,\n",
    "        \"f1_macro\": f1,\n",
    "        \"confusion_matrix\": conf_matrix\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ragas score\n",
    "# load_dotenv()\n",
    "\n",
    "# groq = os.getenv(\"groq_key\")\n",
    "\n",
    "# evaluator_llm = LangchainLLMWrapper(ChatGroq(temperature=0, model=\"llama-3.1-70b-versatile\", api_key=groq))\n",
    "# evaluator_embeddings = LangchainEmbeddingsWrapper(HuggingFaceEmbeddings(model_name=\"NeuML/pubmedbert-base-embeddings\"))\n",
    "\n",
    "# def calculate_RAGAS_answer(generated_answer, gold_answer, question, context):\n",
    "#   data = {'question': [question],\n",
    "#           'contexts': [context],\n",
    "#           'answer': [generated_answer],\n",
    "#           'ground_truth': [gold_answer]}\n",
    "#   dataset = Dataset.from_dict(data)\n",
    "#   score = evaluate(dataset, llm=evaluator_llm, embeddings=evaluator_embeddings, metrics = [faithfulness, answer_relevancy, context_precision, context_recall]).to_pandas()\n",
    "#   faithfulness_score = score['faithfulness'].iloc[0]\n",
    "#   answer_relevancy_score = score['answer_relevancy'].iloc[0]\n",
    "#   context_precision_score = score['context_precision'].iloc[0]\n",
    "#   context_recall_score = score['context_recall'].iloc[0]\n",
    "#   return faithfulness_score, answer_relevancy_score, context_precision_score, context_recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from extraction import llmAgent\n",
    "# from ppi_deprescribe import merge_results\n",
    "\n",
    "# llm_agent = llmAgent(groq_key=groq, data_path=path)\n",
    "\n",
    "# data = pd.read_csv(path / 'LabeledResponses_ReturnedResponse.csv')\n",
    "\n",
    "# ragas_results_df = pd.DataFrame(columns=['key', 'ragas_score'])\n",
    "\n",
    "# for key in data['key']:\n",
    "#     print(f\"Starting key {key}\")\n",
    "#     start = time.time()\n",
    "\n",
    "#     temp = data[data['key'] == key]\n",
    "#     temp = temp.reset_index()\n",
    "\n",
    "#     results_dict = {\n",
    "#         \"diagnosis_dict\": llm_agent.extract_diagnosis(encounter_key=key),\n",
    "#         \"encounter_dict\": llm_agent.extract_encounter_info(encounter_key=key),\n",
    "#         \"notes_dict\": llm_agent.extract_notes(encounter_key=key),\n",
    "#     }\n",
    "#     final_dict = merge_results(results_dict=results_dict)\n",
    "\n",
    "#     final_reasoning = llm_agent.summarize_reasonings(results_dict=results_dict)\n",
    "\n",
    "#     generated_answer = temp['Reasoning'][0]\n",
    "#     gold_answer = temp['GS_response'][0]\n",
    "#     question = \"You are a knowledgeable medical provider who specializes in medication management. Given a list of diagnosis and some snippets from patients notes, answer if the patient notes contain any of the diagnosis. Based on the information from the note context, does the patient have any of the following: 1. Mild to moderate esophagitis 2. GERD 3. Peptic Ulcer Disease 4. Upper GI symptoms 5. ICU Stress Ulcer Prophylaxis 6. Barretts Esophagus 7. Chronic NSAID use with bleeding risk 8. Severe esophagitis 9. Documented history of bleeding GI ulcer 10. H pylori infection 11. Explain the reasoning for your answer. Return the answer for each of these as a formatted JSON object with the key being the condition and the value being a boolean value for the first 10.  For the final question, return a string with the reasoning for your answer. Summarize the reasonings from the three sources. You are a knowledgeable medical provider who specializes in medication management. Based on the following json files, please provide a single explanation of the reasoning given by the 'Reasoning' key. Summarize given equal weight to each. Do not add any additional information, only summarize what is given.\" \n",
    "#     context = [final_reasoning]\n",
    "\n",
    "#     ragas_score = calculate_RAGAS_answer(generated_answer, gold_answer, question, context)\n",
    "\n",
    "#     print(ragas_score)\n",
    "#     print(f\"{key} took {time.time() - start} seconds to process.\")\n",
    "\n",
    "#     new_row = pd.DataFrame([{'key': key, 'ragas_score': ragas_score}])\n",
    "#     ragas_results_df = pd.concat([ragas_results_df, new_row], ignore_index=True)\n",
    "\n",
    "# ragas_results_df.to_csv(path / 'model_results.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "groq = os.getenv(\"groq_key\")\n",
    "path = Path(os.getenv(\"data_path\"))\n",
    "\n",
    "data = pd.read_csv(path / 'LabeledResponses.csv')\n",
    "\n",
    "results_df = pd.DataFrame(columns=['key', 'rec', 'response'])\n",
    "\n",
    "for key in data['key']:\n",
    "    start = time.time()\n",
    "    print(f\"Starting key: {key}.\")\n",
    "    response = main(groq, path, key)\n",
    "    new_row = pd.DataFrame([{'key': key, 'rec': response[0], 'response': response[1]}])\n",
    "    results_df = pd.concat([results_df, new_row], ignore_index=True)\n",
    "    print(f\"{key} took {time.time() - start} seconds to process.\")\n",
    "\n",
    "results_df.to_csv(path / 'model_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\krisr\\AppData\\Local\\Temp\\ipykernel_25640\\509916528.py:52: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results_df = pd.concat([results_df, new_row], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D6253A5CE371EA took 7.657536506652832 seconds to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DFAFED1811B871 took 1.948941707611084 seconds to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D4865B8BBB294E took 1.4121620655059814 seconds to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D00F3A8D5F43B2 took 1.52909517288208 seconds to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DA15CCF42ACF8B took 1.357126235961914 seconds to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DFB07E6B8F0957 took 1.4750165939331055 seconds to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D1BD3665C06499 took 1.4937725067138672 seconds to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D068E26FFF9F43 took 1.5222315788269043 seconds to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D0E44FD0BBD96F took 1.5302648544311523 seconds to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D17AFAD1D7BB68 took 1.513643503189087 seconds to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DAA6DE9D2EC973 took 1.425734281539917 seconds to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D5A7045ED60A2E took 1.4647469520568848 seconds to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D4BAEF91CA7588 took 1.4273855686187744 seconds to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DACBDED2F35C67 took 1.530454158782959 seconds to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DA44DBBF3CCE49 took 1.5280311107635498 seconds to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DAABF28BCE02C7 took 1.4010019302368164 seconds to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DB1D9E08BBEC87 took 1.3481874465942383 seconds to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D6F69E18CCF3AE took 1.3765113353729248 seconds to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D35D002F7350C0 took 1.4789600372314453 seconds to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF655634638393 took 1.4913709163665771 seconds to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D8204A1E91DE84 took 1.4044029712677002 seconds to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DD952F0D84745E took 1.495049238204956 seconds to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D68D537D96A1B6 took 1.4528133869171143 seconds to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DBAA0A10220A8D took 1.9000639915466309 seconds to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D267D1234A0576 took 1.4332771301269531 seconds to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DC2661C3B5DC2A took 1.4215540885925293 seconds to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D57176E8D660E4 took 1.3547935485839844 seconds to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D188F3E1578728 took 1.5435926914215088 seconds to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D6C6E04C56E5D7 took 1.4965670108795166 seconds to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF66490AD8A6B6 took 1.5245029926300049 seconds to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D02DC627A3743A took 1.4305224418640137 seconds to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D01FCB9EFFD6D5 took 1.5144269466400146 seconds to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D03868A75BBE62 took 1.437124252319336 seconds to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D038C6CE0A2867 took 1.3705525398254395 seconds to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D03999FEAA1FCD took 1.4846932888031006 seconds to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D67731C3136779 took 1.5090696811676025 seconds to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D5A533360225FF took 1.4227116107940674 seconds to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SyntheticPt1 took 1.2180049419403076 seconds to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SyntheticPt2 took 1.368086814880371 seconds to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SyntheticPt3 took 1.2231299877166748 seconds to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SyntheticPt4 took 1.4568538665771484 seconds to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SyntheticPt5 took 1.4689781665802002 seconds to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SyntheticPt6 took 1.9591114521026611 seconds to process.\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "path = Path(os.getenv(\"data_path\"))\n",
    "\n",
    "rouge1_scorer = rouge_scorer.RougeScorer(['rouge1'], use_stemmer=True)\n",
    "rouge2_scorer = rouge_scorer.RougeScorer(['rouge2'], use_stemmer=True)\n",
    "rougeL_scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "\n",
    "smoothing_function = SmoothingFunction().method1\n",
    "\n",
    "# data = pd.read_csv(path / 'summary_run_results.csv')\n",
    "# data = pd.read_csv(path / 'summary_with_notes_run_results.csv')\n",
    "data = pd.read_csv(path / 'baseline_eval.csv')\n",
    "\n",
    "results_df = pd.DataFrame(columns=['key', 'GS_response', 'ret_reasoning', 'bert_Precision', 'bert_Recall', 'bert_F1',\n",
    "                                   'rouge1_Precision', 'rouge1_Recall', 'rouge1_F1', \n",
    "                                   'rouge2_Precision', 'rouge2_Recall', 'rouge2_F1', \n",
    "                                   'rougeL_Precision', 'rougeL_Recall', 'rougeL_F1', \n",
    "                                   'METEOR_score', 'bleu_score'\n",
    "                                   ])\n",
    "\n",
    "data = data[['key', 'GS_response', 'ret_reasoning']]\n",
    "\n",
    "for key in data['key']:\n",
    "    start = time.time()\n",
    "    temp = data[data['key'] == key].reset_index()\n",
    "\n",
    "    # Compute BERTScore\n",
    "    bert_P, bert_R, bert_F1 = score([temp['ret_reasoning'][0]], [temp['GS_response'][0]], lang=\"en\", verbose=False)\n",
    "\n",
    "    # Compute Rouge scores\n",
    "    rouge1_scores = rouge1_scorer.score(temp['GS_response'][0], temp['ret_reasoning'][0])\n",
    "    rouge2_scores = rouge2_scorer.score(temp['GS_response'][0], temp['ret_reasoning'][0])\n",
    "    rougeL_scores = rougeL_scorer.score(temp['GS_response'][0], temp['ret_reasoning'][0])\n",
    "\n",
    "    response_tokens = word_tokenize(temp['ret_reasoning'][0])\n",
    "    gs_response_tokens = word_tokenize(temp['GS_response'][0])\n",
    "\n",
    "    # Compute METEOR score\n",
    "    score_from_meteor = meteor_score([gs_response_tokens], response_tokens)\n",
    "\n",
    "    # Compute Bleu score\n",
    "    bleu_score = sentence_bleu(gs_response_tokens, response_tokens, smoothing_function=smoothing_function)\n",
    "\n",
    "    new_row = pd.DataFrame([{'key': key, 'GS_response': temp['GS_response'][0], 'ret_reasoning': temp['ret_reasoning'][0],\n",
    "                            'bert_Precision': bert_P.item(), 'bert_Recall': bert_R.item(), 'bert_F1': bert_F1.item(), \n",
    "                            'rouge1_Precision': rouge1_scores['rouge1'].precision, 'rouge1_Recall': rouge1_scores['rouge1'].recall, 'rouge1_F1': rouge1_scores['rouge1'].fmeasure, \n",
    "                            'rouge2_Precision': rouge2_scores['rouge2'].precision, 'rouge2_Recall': rouge2_scores['rouge2'].recall, 'rouge2_F1': rouge2_scores['rouge2'].fmeasure, \n",
    "                            'rougeL_Precision': rougeL_scores['rougeL'].precision, 'rougeL_Recall': rougeL_scores['rougeL'].recall, 'rougeL_F1': rougeL_scores['rougeL'].fmeasure, \n",
    "                            'METEOR_score': score_from_meteor, 'bleu_score': bleu_score\n",
    "                            }])\n",
    "    results_df = pd.concat([results_df, new_row], ignore_index=True)\n",
    "    print(f\"{key} took {time.time() - start} seconds to process.\")\n",
    "\n",
    "# results_df.to_csv(path / 'summary_results.csv', index=False)\n",
    "# results_df.to_csv(path / 'summary_with_notes_results.csv', index=False)\n",
    "results_df.to_csv(path / 'baseline_results.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline results:\n",
      "Average scores: \n",
      "bert_Precision      0.834044\n",
      "bert_Recall         0.838979\n",
      "bert_F1             0.836331\n",
      "rouge1_Precision    0.166015\n",
      "rouge1_Recall       0.281776\n",
      "rouge1_F1           0.201841\n",
      "rouge2_Precision    0.028220\n",
      "rouge2_Recall       0.043931\n",
      "rouge2_F1           0.033558\n",
      "rougeL_Precision    0.099091\n",
      "rougeL_Recall       0.169337\n",
      "rougeL_F1           0.120264\n",
      "METEOR_score        0.157428\n",
      "bleu_score          0.004864\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "path = Path(os.getenv(\"data_path\"))\n",
    "\n",
    "# summary_results = pd.read_csv(path / 'summary_results.csv')\n",
    "# summary_with_notes_results = pd.read_csv(path / 'summary_with_notes_results.csv')\n",
    "baseline_results = pd.read_csv(path / 'baseline_results.csv')\n",
    "\n",
    "# summary_results = summary_results.drop([\"key\", \"GS_response\", \"ret_reasoning\"], axis=1)\n",
    "# summary_with_notes_results = summary_with_notes_results.drop([\"key\", \"GS_response\", \"ret_reasoning\"], axis=1)\n",
    "baseline_results = baseline_results.drop([\"key\", \"GS_response\", \"ret_reasoning\"], axis=1)\n",
    "\n",
    "# min_summary_results = summary_results.min()\n",
    "# max_summary_results = summary_results.max()\n",
    "# med_summary_results = summary_results.median()\n",
    "# avg_summary_results = summary_results.mean()\n",
    "\n",
    "avg_baseline_results = baseline_results.mean()\n",
    "\n",
    "# print(\"Summary results:\")\n",
    "# print(f\"Minimum scores: \\n{min_summary_results}\")\n",
    "# print(f\"Maximum scores: \\n{max_summary_results}\")\n",
    "# print(f\"Median scores: \\n{med_summary_results}\")\n",
    "# print(f\"Average scores: \\n{avg_summary_results}\")\n",
    "\n",
    "# min_summary_with_notes_results = summary_with_notes_results.min()\n",
    "# max_summary_with_notes_results = summary_with_notes_results.max()\n",
    "# med_summary_with_notes_results = summary_with_notes_results.median()\n",
    "# avg_summary_with_notes_results = summary_with_notes_results.mean()\n",
    "\n",
    "# print(\"Summary with notes results:\")\n",
    "# print(f\"Minimum scores: \\n{min_summary_with_notes_results}\")\n",
    "# print(f\"Maximum scores: \\n{max_summary_with_notes_results}\")\n",
    "# print(f\"Median scores: \\n{med_summary_with_notes_results}\")\n",
    "# print(f\"Average scores: \\n{avg_summary_with_notes_results}\")\n",
    "\n",
    "print(\"Baseline results:\")\n",
    "print(f\"Average scores: \\n{avg_baseline_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# load data\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(path \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLabeledResponses_ReturnedResponse.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m y_true \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecommendation\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      5\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRec_Returned\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "data = pd.read_csv(path / 'LabeledResponses_ReturnedResponse.csv')\n",
    "\n",
    "y_true = data['recommendation']\n",
    "y_pred = data['Rec_Returned']\n",
    "class_labels = ['continue', 'deprescribe', 'stop']\n",
    "\n",
    "# Evaluate the model\n",
    "metrics = evaluate_multiclass_classification(y_true, y_pred, class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
