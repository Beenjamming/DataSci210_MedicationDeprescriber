{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\krisr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\krisr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\krisr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "# from langchain_groq import ChatGroq\n",
    "# from ragas import evaluate\n",
    "# from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "# from ragas.llms import LangchainLLMWrapper\n",
    "# from ragas.metrics import answer_relevancy, faithfulness, context_recall, context_precision\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "# from datasets import Dataset\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_recall_fscore_support\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "import pandas as pd\n",
    "# import requests\n",
    "import time\n",
    "from pathlib import Path\n",
    "from main import main\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_multiclass_classification(y_true, y_pred, class_labels):\n",
    "    \"\"\"\n",
    "    Evaluates a multiclass classification model.\n",
    "    \n",
    "    y_true: Ground truth labels\n",
    "    y_pred: Predicted labels from the classifier\n",
    "    class_labels: List of class names\n",
    "    \n",
    "    Returns a dictionary of accuracy, precision, recall, F1, and confusion matrix.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Accuracy\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    # Precision, Recall, F1-score\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='macro')  # macro-averaging\n",
    "    \n",
    "    # Classification report (optional detailed breakdown for each class)\n",
    "    class_report = classification_report(y_true, y_pred, target_names=class_labels)\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # Print evaluation metrics\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision (Macro Avg): {precision:.4f}\")\n",
    "    print(f\"Recall (Macro Avg): {recall:.4f}\")\n",
    "    print(f\"F1 Score (Macro Avg): {f1:.4f}\")\n",
    "    print(\"\\nClassification Report:\\n\", class_report)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "    \n",
    "    # Return metrics for further analysis if needed\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision_macro\": precision,\n",
    "        \"recall_macro\": recall,\n",
    "        \"f1_macro\": f1,\n",
    "        \"confusion_matrix\": conf_matrix\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ragas score\n",
    "# load_dotenv()\n",
    "\n",
    "# groq = os.getenv(\"groq_key\")\n",
    "\n",
    "# evaluator_llm = LangchainLLMWrapper(ChatGroq(temperature=0, model=\"llama-3.1-70b-versatile\", api_key=groq))\n",
    "# evaluator_embeddings = LangchainEmbeddingsWrapper(HuggingFaceEmbeddings(model_name=\"NeuML/pubmedbert-base-embeddings\"))\n",
    "\n",
    "# def calculate_RAGAS_answer(generated_answer, gold_answer, question, context):\n",
    "#   data = {'question': [question],\n",
    "#           'contexts': [context],\n",
    "#           'answer': [generated_answer],\n",
    "#           'ground_truth': [gold_answer]}\n",
    "#   dataset = Dataset.from_dict(data)\n",
    "#   score = evaluate(dataset, llm=evaluator_llm, embeddings=evaluator_embeddings, metrics = [faithfulness, answer_relevancy, context_precision, context_recall]).to_pandas()\n",
    "#   faithfulness_score = score['faithfulness'].iloc[0]\n",
    "#   answer_relevancy_score = score['answer_relevancy'].iloc[0]\n",
    "#   context_precision_score = score['context_precision'].iloc[0]\n",
    "#   context_recall_score = score['context_recall'].iloc[0]\n",
    "#   return faithfulness_score, answer_relevancy_score, context_precision_score, context_recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from extraction import llmAgent\n",
    "# from ppi_deprescribe import merge_results\n",
    "\n",
    "# llm_agent = llmAgent(groq_key=groq, data_path=path)\n",
    "\n",
    "# data = pd.read_csv(path / 'LabeledResponses_ReturnedResponse.csv')\n",
    "\n",
    "# ragas_results_df = pd.DataFrame(columns=['key', 'ragas_score'])\n",
    "\n",
    "# for key in data['key']:\n",
    "#     print(f\"Starting key {key}\")\n",
    "#     start = time.time()\n",
    "\n",
    "#     temp = data[data['key'] == key]\n",
    "#     temp = temp.reset_index()\n",
    "\n",
    "#     results_dict = {\n",
    "#         \"diagnosis_dict\": llm_agent.extract_diagnosis(encounter_key=key),\n",
    "#         \"encounter_dict\": llm_agent.extract_encounter_info(encounter_key=key),\n",
    "#         \"notes_dict\": llm_agent.extract_notes(encounter_key=key),\n",
    "#     }\n",
    "#     final_dict = merge_results(results_dict=results_dict)\n",
    "\n",
    "#     final_reasoning = llm_agent.summarize_reasonings(results_dict=results_dict)\n",
    "\n",
    "#     generated_answer = temp['Reasoning'][0]\n",
    "#     gold_answer = temp['GS_response'][0]\n",
    "#     question = \"You are a knowledgeable medical provider who specializes in medication management. Given a list of diagnosis and some snippets from patients notes, answer if the patient notes contain any of the diagnosis. Based on the information from the note context, does the patient have any of the following: 1. Mild to moderate esophagitis 2. GERD 3. Peptic Ulcer Disease 4. Upper GI symptoms 5. ICU Stress Ulcer Prophylaxis 6. Barretts Esophagus 7. Chronic NSAID use with bleeding risk 8. Severe esophagitis 9. Documented history of bleeding GI ulcer 10. H pylori infection 11. Explain the reasoning for your answer. Return the answer for each of these as a formatted JSON object with the key being the condition and the value being a boolean value for the first 10.  For the final question, return a string with the reasoning for your answer. Summarize the reasonings from the three sources. You are a knowledgeable medical provider who specializes in medication management. Based on the following json files, please provide a single explanation of the reasoning given by the 'Reasoning' key. Summarize given equal weight to each. Do not add any additional information, only summarize what is given.\" \n",
    "#     context = [final_reasoning]\n",
    "\n",
    "#     ragas_score = calculate_RAGAS_answer(generated_answer, gold_answer, question, context)\n",
    "\n",
    "#     print(ragas_score)\n",
    "#     print(f\"{key} took {time.time() - start} seconds to process.\")\n",
    "\n",
    "#     new_row = pd.DataFrame([{'key': key, 'ragas_score': ragas_score}])\n",
    "#     ragas_results_df = pd.concat([ragas_results_df, new_row], ignore_index=True)\n",
    "\n",
    "# ragas_results_df.to_csv(path / 'model_results.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "groq = os.getenv(\"groq_key\")\n",
    "path = Path(os.getenv(\"data_path\"))\n",
    "\n",
    "data = pd.read_csv(path / 'LabeledResponses.csv')\n",
    "\n",
    "results_df = pd.DataFrame(columns=['key', 'rec', 'response'])\n",
    "\n",
    "for key in data['key']:\n",
    "    start = time.time()\n",
    "    print(f\"Starting key: {key}.\")\n",
    "    response = main(groq, path, key)\n",
    "    new_row = pd.DataFrame([{'key': key, 'rec': response[0], 'response': response[1]}])\n",
    "    results_df = pd.concat([results_df, new_row], ignore_index=True)\n",
    "    print(f\"{key} took {time.time() - start} seconds to process.\")\n",
    "\n",
    "results_df.to_csv(path / 'model_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\krisr\\AppData\\Local\\Temp\\ipykernel_33856\\3594601514.py:51: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results_df = pd.concat([results_df, new_row], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D267D1234A0576 took 0.8580467700958252 seconds to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DC2661C3B5DC2A took 0.973677396774292 seconds to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D57176E8D660E4 took 1.3716905117034912 seconds to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D188F3E1578728 took 0.9806826114654541 seconds to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D6C6E04C56E5D7 took 0.870643138885498 seconds to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF66490AD8A6B6 took 1.1539711952209473 seconds to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D02DC627A3743A took 1.7403254508972168 seconds to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D01FCB9EFFD6D5 took 2.6330106258392334 seconds to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D03868A75BBE62 took 2.6140735149383545 seconds to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D038C6CE0A2867 took 2.6398394107818604 seconds to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D03999FEAA1FCD took 1.366445779800415 seconds to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D67731C3136779 took 1.9281206130981445 seconds to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D5A533360225FF took 1.9104681015014648 seconds to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SyntheticPt1 took 2.0660667419433594 seconds to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SyntheticPt2 took 1.7030436992645264 seconds to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SyntheticPt3 took 1.7484714984893799 seconds to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SyntheticPt4 took 1.6993508338928223 seconds to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SyntheticPt5 took 1.4208638668060303 seconds to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D6253A5CE371EA took 1.883615493774414 seconds to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DFAFED1811B871 took 1.4437098503112793 seconds to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D4865B8BBB294E took 1.9608826637268066 seconds to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D00F3A8D5F43B2 took 1.749502420425415 seconds to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DA15CCF42ACF8B took 1.1647562980651855 seconds to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DFB07E6B8F0957 took 1.7646265029907227 seconds to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D1BD3665C06499 took 1.6673924922943115 seconds to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D068E26FFF9F43 took 1.47794508934021 seconds to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D0E44FD0BBD96F took 1.5063502788543701 seconds to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D17AFAD1D7BB68 took 1.4694128036499023 seconds to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DAA6DE9D2EC973 took 1.6621880531311035 seconds to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D5A7045ED60A2E took 1.434016227722168 seconds to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D4BAEF91CA7588 took 1.9470727443695068 seconds to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DACBDED2F35C67 took 1.4344549179077148 seconds to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DA44DBBF3CCE49 took 1.5844650268554688 seconds to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DAABF28BCE02C7 took 1.6261992454528809 seconds to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DB1D9E08BBEC87 took 1.6492855548858643 seconds to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D6F69E18CCF3AE took 1.534773826599121 seconds to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D35D002F7350C0 took 1.729513168334961 seconds to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF655634638393 took 1.5584428310394287 seconds to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D8204A1E91DE84 took 1.2011876106262207 seconds to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DD952F0D84745E took 2.1129047870635986 seconds to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D68D537D96A1B6 took 1.445019245147705 seconds to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DBAA0A10220A8D took 1.198911190032959 seconds to process.\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "path = Path(os.getenv(\"data_path\"))\n",
    "\n",
    "rouge1_scorer = rouge_scorer.RougeScorer(['rouge1'], use_stemmer=True)\n",
    "rouge2_scorer = rouge_scorer.RougeScorer(['rouge2'], use_stemmer=True)\n",
    "rougeL_scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "\n",
    "smoothing_function = SmoothingFunction().method1\n",
    "\n",
    "# data = pd.read_csv(path / 'summary_run_results.csv')\n",
    "data = pd.read_csv(path / 'summary_with_notes_run_results.csv')\n",
    "\n",
    "results_df = pd.DataFrame(columns=['key', 'GS_response', 'ret_reasoning', 'bert_Precision', 'bert_Recall', 'bert_F1',\n",
    "                                   'rouge1_Precision', 'rouge1_Recall', 'rouge1_F1', \n",
    "                                   'rouge2_Precision', 'rouge2_Recall', 'rouge2_F1', \n",
    "                                   'rougeL_Precision', 'rougeL_Recall', 'rougeL_F1', \n",
    "                                   'METEOR_score', 'bleu_score'\n",
    "                                   ])\n",
    "\n",
    "data = data[['key', 'GS_response', 'ret_reasoning']]\n",
    "\n",
    "for key in data['key']:\n",
    "    start = time.time()\n",
    "    temp = data[data['key'] == key].reset_index()\n",
    "\n",
    "    # Compute BERTScore\n",
    "    bert_P, bert_R, bert_F1 = score([temp['ret_reasoning'][0]], [temp['GS_response'][0]], lang=\"en\", verbose=False)\n",
    "\n",
    "    # Compute Rouge scores\n",
    "    rouge1_scores = rouge1_scorer.score(temp['GS_response'][0], temp['ret_reasoning'][0])\n",
    "    rouge2_scores = rouge2_scorer.score(temp['GS_response'][0], temp['ret_reasoning'][0])\n",
    "    rougeL_scores = rougeL_scorer.score(temp['GS_response'][0], temp['ret_reasoning'][0])\n",
    "\n",
    "    response_tokens = word_tokenize(temp['ret_reasoning'][0])\n",
    "    gs_response_tokens = word_tokenize(temp['GS_response'][0])\n",
    "\n",
    "    # Compute METEOR score\n",
    "    score_from_meteor = meteor_score([gs_response_tokens], response_tokens)\n",
    "\n",
    "    # Compute Bleu score\n",
    "    bleu_score = sentence_bleu(gs_response_tokens, response_tokens, smoothing_function=smoothing_function)\n",
    "\n",
    "    new_row = pd.DataFrame([{'key': key, 'GS_response': temp['GS_response'][0], 'ret_reasoning': temp['ret_reasoning'][0],\n",
    "                            'bert_Precision': bert_P.item(), 'bert_Recall': bert_R.item(), 'bert_F1': bert_F1.item(), \n",
    "                            'rouge1_Precision': rouge1_scores['rouge1'].precision, 'rouge1_Recall': rouge1_scores['rouge1'].recall, 'rouge1_F1': rouge1_scores['rouge1'].fmeasure, \n",
    "                            'rouge2_Precision': rouge2_scores['rouge2'].precision, 'rouge2_Recall': rouge2_scores['rouge2'].recall, 'rouge2_F1': rouge2_scores['rouge2'].fmeasure, \n",
    "                            'rougeL_Precision': rougeL_scores['rougeL'].precision, 'rougeL_Recall': rougeL_scores['rougeL'].recall, 'rougeL_F1': rougeL_scores['rougeL'].fmeasure, \n",
    "                            'METEOR_score': score_from_meteor, 'bleu_score': bleu_score\n",
    "                            }])\n",
    "    results_df = pd.concat([results_df, new_row], ignore_index=True)\n",
    "    print(f\"{key} took {time.time() - start} seconds to process.\")\n",
    "\n",
    "# results_df.to_csv(path / 'summary_results.csv', index=False)\n",
    "results_df.to_csv(path / 'summary_with_notes_results.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary results:\n",
      "Average scores: \n",
      "bert_Precision      0.844873\n",
      "bert_Recall         0.826221\n",
      "bert_F1             0.834947\n",
      "rouge1_Precision    0.249225\n",
      "rouge1_Recall       0.237062\n",
      "rouge1_F1           0.192426\n",
      "rouge2_Precision    0.043723\n",
      "rouge2_Recall       0.049707\n",
      "rouge2_F1           0.036188\n",
      "rougeL_Precision    0.161911\n",
      "rougeL_Recall       0.147335\n",
      "rougeL_F1           0.118974\n",
      "METEOR_score        0.148167\n",
      "bleu_score          0.004459\n",
      "dtype: float64\n",
      "Summary with notes results:\n",
      "Average scores: \n",
      "bert_Precision      0.843484\n",
      "bert_Recall         0.826848\n",
      "bert_F1             0.834554\n",
      "rouge1_Precision    0.251410\n",
      "rouge1_Recall       0.241275\n",
      "rouge1_F1           0.193580\n",
      "rouge2_Precision    0.046370\n",
      "rouge2_Recall       0.047106\n",
      "rouge2_F1           0.034643\n",
      "rougeL_Precision    0.162364\n",
      "rougeL_Recall       0.142907\n",
      "rougeL_F1           0.114941\n",
      "METEOR_score        0.142836\n",
      "bleu_score          0.004213\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "path = Path(os.getenv(\"data_path\"))\n",
    "\n",
    "summary_results = pd.read_csv(path / 'summary_results.csv')\n",
    "summary_with_notes_results = pd.read_csv(path / 'summary_with_notes_results.csv')\n",
    "\n",
    "summary_results = summary_results.drop([\"key\", \"GS_response\", \"ret_reasoning\"], axis=1)\n",
    "summary_with_notes_results = summary_with_notes_results.drop([\"key\", \"GS_response\", \"ret_reasoning\"], axis=1)\n",
    "\n",
    "# min_summary_results = summary_results.min()\n",
    "# max_summary_results = summary_results.max()\n",
    "# med_summary_results = summary_results.median()\n",
    "avg_summary_results = summary_results.mean()\n",
    "\n",
    "print(\"Summary results:\")\n",
    "# print(f\"Minimum scores: \\n{min_summary_results}\")\n",
    "# print(f\"Maximum scores: \\n{max_summary_results}\")\n",
    "# print(f\"Median scores: \\n{med_summary_results}\")\n",
    "print(f\"Average scores: \\n{avg_summary_results}\")\n",
    "\n",
    "# min_summary_with_notes_results = summary_with_notes_results.min()\n",
    "# max_summary_with_notes_results = summary_with_notes_results.max()\n",
    "# med_summary_with_notes_results = summary_with_notes_results.median()\n",
    "avg_summary_with_notes_results = summary_with_notes_results.mean()\n",
    "\n",
    "print(\"Summary with notes results:\")\n",
    "# print(f\"Minimum scores: \\n{min_summary_with_notes_results}\")\n",
    "# print(f\"Maximum scores: \\n{max_summary_with_notes_results}\")\n",
    "# print(f\"Median scores: \\n{med_summary_with_notes_results}\")\n",
    "print(f\"Average scores: \\n{avg_summary_with_notes_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# load data\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(path \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLabeledResponses_ReturnedResponse.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m y_true \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecommendation\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      5\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRec_Returned\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "data = pd.read_csv(path / 'LabeledResponses_ReturnedResponse.csv')\n",
    "\n",
    "y_true = data['recommendation']\n",
    "y_pred = data['Rec_Returned']\n",
    "class_labels = ['continue', 'deprescribe', 'stop']\n",
    "\n",
    "# Evaluate the model\n",
    "metrics = evaluate_multiclass_classification(y_true, y_pred, class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
