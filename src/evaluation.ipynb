{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_groq import ChatGroq\n",
    "# from ragas import evaluate\n",
    "# from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "# from ragas.llms import LangchainLLMWrapper\n",
    "# from ragas.metrics import answer_relevancy, faithfulness, context_recall, context_precision\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "# from datasets import Dataset\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_recall_fscore_support\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_multiclass_classification(y_true, y_pred, class_labels):\n",
    "    \"\"\"\n",
    "    Evaluates a multiclass classification model.\n",
    "    \n",
    "    y_true: Ground truth labels\n",
    "    y_pred: Predicted labels from the classifier\n",
    "    class_labels: List of class names\n",
    "    \n",
    "    Returns a dictionary of accuracy, precision, recall, F1, and confusion matrix.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Accuracy\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    # Precision, Recall, F1-score\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='macro')  # macro-averaging\n",
    "    \n",
    "    # Classification report (optional detailed breakdown for each class)\n",
    "    class_report = classification_report(y_true, y_pred, target_names=class_labels)\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # Print evaluation metrics\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision (Macro Avg): {precision:.4f}\")\n",
    "    print(f\"Recall (Macro Avg): {recall:.4f}\")\n",
    "    print(f\"F1 Score (Macro Avg): {f1:.4f}\")\n",
    "    print(\"\\nClassification Report:\\n\", class_report)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "    \n",
    "    # Return metrics for further analysis if needed\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision_macro\": precision,\n",
    "        \"recall_macro\": recall,\n",
    "        \"f1_macro\": f1,\n",
    "        \"confusion_matrix\": conf_matrix\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\krisr\\AppData\\Local\\Temp\\ipykernel_30464\\3110027476.py:6: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  evaluator_embeddings = LangchainEmbeddingsWrapper(HuggingFaceEmbeddings(model_name=\"NeuML/pubmedbert-base-embeddings\"))\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "groq = os.getenv(\"groqkey\")\n",
    "\n",
    "evaluator_llm = LangchainLLMWrapper(ChatGroq(temperature=0, model=\"llama-3.1-70b-versatile\", api_key=groq))\n",
    "evaluator_embeddings = LangchainEmbeddingsWrapper(HuggingFaceEmbeddings(model_name=\"NeuML/pubmedbert-base-embeddings\"))\n",
    "\n",
    "def calculate_RAGAS_answer(generated_answer, gold_answer, question, context):\n",
    "  data = {'question': [question],\n",
    "          'contexts': [context],\n",
    "          'answer': [generated_answer],\n",
    "          'ground_truth': [gold_answer]}\n",
    "  dataset = Dataset.from_dict(data)\n",
    "  score = evaluate(dataset, llm=evaluator_llm, embeddings=evaluator_embeddings, metrics = [faithfulness, answer_relevancy, context_precision, context_recall]).to_pandas()\n",
    "  faithfulness_score = score['faithfulness'].iloc[0]\n",
    "  answer_relevancy_score = score['answer_relevancy'].iloc[0]\n",
    "  context_precision_score = score['context_precision'].iloc[0]\n",
    "  context_recall_score = score['context_recall'].iloc[0]\n",
    "  return faithfulness_score, answer_relevancy_score, context_precision_score, context_recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from fastapi import FastAPI\n",
    "from fastapi.responses import HTMLResponse\n",
    "import uvicorn\n",
    "from IPython.display import Markdown, display\n",
    "from extraction import llmAgent\n",
    "from ppi_deprescribe import merge_results, ppi_deprescribe\n",
    "import os \n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "path = Path(os.getenv(\"data_path\"))\n",
    "groq = os.getenv(\"groqkey\")\n",
    "\n",
    "def deprescribe(key):\n",
    "    # extract information\n",
    "    llm_agent = llmAgent(groq_key=groq, data_path=path)\n",
    "\n",
    "    results_dict = {\n",
    "        \"diagnosis_dict\": llm_agent.extract_diagnosis(encounter_key=key),\n",
    "        \"encounter_dict\": llm_agent.extract_encounter_info(encounter_key=key),\n",
    "        # Is the reasoning in the json or sepearte?\n",
    "        # Should the reasoning be included in any of them or just the diangosis with the reasoning seperate?\n",
    "        \"notes_dict\": llm_agent.extract_notes(encounter_key=key),\n",
    "    }\n",
    "    print(results_dict['notes_dict'])\n",
    "    # # #   master formatter step   # # #\n",
    "    # merge the diagnosis booleans (just use OR logic for now)\n",
    "    # make a final \"reasoning\" behind the recommendation\n",
    "    final_dict = merge_results(results_dict=results_dict)\n",
    "\n",
    "    # feed the three reasonings to LLM to get a single summary\n",
    "    final_reasoning = llm_agent.summarize_reasonings(results_dict=results_dict)\n",
    "\n",
    "    # # #   get recommendation from PPI algorithm   # # #\n",
    "    recommendation_str = ppi_deprescribe(patient_diagnosis=final_dict)\n",
    "    return recommendation_str, final_reasoning\n",
    "    #print(\"Recommendation: \")\n",
    "    #print(recommendation_str)\n",
    "    #print(\"\\nReasoning: \")\n",
    "    #print(final_reasoning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "data = pd.read_csv(path / 'LabeledResponses.csv')\n",
    "\n",
    "results_df = pd.DataFrame(columns=['key', 'rec', 'response'])\n",
    "\n",
    "for key in data['key']:\n",
    "    start = time.time()\n",
    "    print(f\"Starting key: {key}.\")\n",
    "    response = deprescribe(key)\n",
    "    new_row = pd.DataFrame([{'key': key, 'rec': response[0], 'response': response[1]}])\n",
    "    results_df = pd.concat([results_df, new_row], ignore_index=True)\n",
    "    print(f\"{key} took {time.time() - start} seconds to process.\")\n",
    "\n",
    "results_df.to_csv(path / 'model_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "data = pd.read_csv(path / 'LabeledResponses_ReturnedResponse.csv')\n",
    "\n",
    "y_true = data['recommendation']\n",
    "y_pred = data['Rec_Returned']\n",
    "class_labels = ['continue', 'deprescribe', 'stop']\n",
    "\n",
    "# Evaluate the model\n",
    "metrics = evaluate_multiclass_classification(y_true, y_pred, class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from extraction import llmAgent\n",
    "from ppi_deprescribe import merge_results\n",
    "\n",
    "llm_agent = llmAgent(groq_key=groq, data_path=path)\n",
    "\n",
    "data = pd.read_csv(path / 'LabeledResponses_ReturnedResponse.csv')\n",
    "\n",
    "ragas_results_df = pd.DataFrame(columns=['key', 'ragas_score'])\n",
    "\n",
    "for key in data['key']:\n",
    "    print(f\"Starting key {key}\")\n",
    "    start = time.time()\n",
    "\n",
    "    temp = data[data['key'] == key]\n",
    "    temp = temp.reset_index()\n",
    "\n",
    "    results_dict = {\n",
    "        \"diagnosis_dict\": llm_agent.extract_diagnosis(encounter_key=key),\n",
    "        \"encounter_dict\": llm_agent.extract_encounter_info(encounter_key=key),\n",
    "        \"notes_dict\": llm_agent.extract_notes(encounter_key=key),\n",
    "    }\n",
    "    final_dict = merge_results(results_dict=results_dict)\n",
    "\n",
    "    final_reasoning = llm_agent.summarize_reasonings(results_dict=results_dict)\n",
    "\n",
    "    generated_answer = temp['Reasoning'][0]\n",
    "    gold_answer = temp['GS_response'][0]\n",
    "    question = \"You are a knowledgeable medical provider who specializes in medication management. Given a list of diagnosis and some snippets from patients notes, answer if the patient notes contain any of the diagnosis. Based on the information from the note context, does the patient have any of the following: 1. Mild to moderate esophagitis 2. GERD 3. Peptic Ulcer Disease 4. Upper GI symptoms 5. ICU Stress Ulcer Prophylaxis 6. Barretts Esophagus 7. Chronic NSAID use with bleeding risk 8. Severe esophagitis 9. Documented history of bleeding GI ulcer 10. H pylori infection 11. Explain the reasoning for your answer. Return the answer for each of these as a formatted JSON object with the key being the condition and the value being a boolean value for the first 10.  For the final question, return a string with the reasoning for your answer. Summarize the reasonings from the three sources. You are a knowledgeable medical provider who specializes in medication management. Based on the following json files, please provide a single explanation of the reasoning given by the 'Reasoning' key. Summarize given equal weight to each. Do not add any additional information, only summarize what is given.\" \n",
    "    context = [final_reasoning]\n",
    "\n",
    "    ragas_score = calculate_RAGAS_answer(generated_answer, gold_answer, question, context)\n",
    "\n",
    "    print(ragas_score)\n",
    "    print(f\"{key} took {time.time() - start} seconds to process.\")\n",
    "\n",
    "    new_row = pd.DataFrame([{'key': key, 'ragas_score': ragas_score}])\n",
    "    ragas_results_df = pd.concat([ragas_results_df, new_row], ignore_index=True)\n",
    "\n",
    "ragas_results_df.to_csv(path / 'model_results.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
