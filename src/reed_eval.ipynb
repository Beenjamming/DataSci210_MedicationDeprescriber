{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_groq import ChatGroq\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from datasets import Dataset\n",
    "from ragas.metrics import answer_relevancy, faithfulness, context_recall, context_precision\n",
    "from ragas import evaluate\n",
    "\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.chat_models import BedrockChat\n",
    "import boto3\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "groq = os.getenv(\"groqkey\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, average_precision_score, classification_report, confusion_matrix, precision_score, precision_recall_fscore_support, recall_score\n",
    "from collections import defaultdict\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import requests\n",
    "\n",
    "def evaluate_multiclass_classification(y_true, y_pred, class_labels):\n",
    "    \"\"\"\n",
    "    Evaluates a multiclass classification model.\n",
    "    \n",
    "    y_true: Ground truth labels\n",
    "    y_pred: Predicted labels from the classifier\n",
    "    class_labels: List of class names\n",
    "    \n",
    "    Returns a dictionary of accuracy, precision, recall, F1, and confusion matrix.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Accuracy\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    # Precision, Recall, F1-score\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='macro')  # macro-averaging\n",
    "    \n",
    "    # Classification report (optional detailed breakdown for each class)\n",
    "    class_report = classification_report(y_true, y_pred, target_names=class_labels)\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # Print evaluation metrics\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision (Macro Avg): {precision:.4f}\")\n",
    "    print(f\"Recall (Macro Avg): {recall:.4f}\")\n",
    "    print(f\"F1 Score (Macro Avg): {f1:.4f}\")\n",
    "    print(\"\\nClassification Report:\\n\", class_report)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "    \n",
    "    # Return metrics for further analysis if needed\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision_macro\": precision,\n",
    "        \"recall_macro\": recall,\n",
    "        \"f1_macro\": f1,\n",
    "        \"confusion_matrix\": conf_matrix\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/reedevans/Documents/mids/DataSci210_MedicationDeprescriber/src/extraction.py:15: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"NeuML/pubmedbert-base-embeddings\")\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D00F3A8D5F43B2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I'm sorry but I do not have the capability to perform this task for you, I am happy to help you with any other queries you may have.\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 32, 'prompt_tokens': 18, 'total_tokens': 50, 'completion_time': 0.099357843, 'prompt_time': 0.00409476, 'queue_time': 0.028999499, 'total_time': 0.103452603}, 'model_name': 'llama3-groq-70b-8192-tool-use-preview', 'system_fingerprint': 'fp_ee4b521143', 'finish_reason': 'stop', 'logprobs': None}, id='run-47ffcf0f-50a3-43f5-9f91-4aa42df0052a-0', usage_metadata={'input_tokens': 18, 'output_tokens': 32, 'total_tokens': 50})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from query import DataLoader as dl\n",
    "from pathlib import Path\n",
    "from extraction import llmAgent\n",
    "\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_groq import ChatGroq\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from datasets import Dataset\n",
    "from ragas.metrics import answer_relevancy, faithfulness, context_recall, context_precision\n",
    "from ragas import evaluate\n",
    "\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.chat_models import BedrockChat\n",
    "import boto3\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "\n",
    "data_path = '../Data'\n",
    "\n",
    "dl = dl(Path(data_path))\n",
    "\n",
    "data = dl.get_labled_encounters()\n",
    "\n",
    "from extraction import llmAgent\n",
    "from ppi_deprescribe import merge_results, ppi_deprescribe\n",
    "\n",
    "\n",
    "key = data.iloc[0]['key']\n",
    "print(key)\n",
    "llm_agent = llmAgent(groq_key=groq, data_path=Path(data_path))\n",
    "\n",
    "llm_agent.llm.invoke(\"what is the capital of the moon?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_diagnosis\n",
      "extractjson\n",
      "<class 'str'>\n",
      "get_encounter\n",
      "extractjson\n",
      "<class 'str'>\n",
      "get_notes\n",
      "extractjson\n",
      "<class 'str'>\n",
      "{'Mild to moderate esophagitis': False, 'GERD': False, 'Peptic Ulcer Disease': False, 'Upper GI symptoms': False, 'ICU Stress Ulcer Prophylaxis': False, 'Barretts Esophagus': False, 'Chronic NSAID use with bleeding risk': False, 'Severe esophagitis': False, 'Documented history of bleeding GI ulcer': False, 'H pylori infection': False, 'Reasoning': 'No evidence of gastrointestinal conditions or bleeding risk factors in the provided information.'}\n",
      "D00F3A8D5F43B2 took 45.08418273925781 seconds to process.\n",
      "get_diagnosis\n",
      "extractjson\n",
      "<class 'str'>\n",
      "get_encounter\n",
      "extractjson\n",
      "<class 'str'>\n",
      "get_notes\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m encounter_dict \u001b[38;5;241m=\u001b[39m llm_agent\u001b[38;5;241m.\u001b[39mextract_encounter_info(encounter_key\u001b[38;5;241m=\u001b[39mkey)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mget_notes\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m notes_dict \u001b[38;5;241m=\u001b[39m \u001b[43mllm_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_notes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencounter_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m results_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdiagnosis_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m: diagnosis_dict,\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencounter_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m: encounter_dict,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnotes_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m: notes_dict,\n\u001b[1;32m     22\u001b[0m }\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(results_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnotes_dict\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/Documents/mids/DataSci210_MedicationDeprescriber/src/extraction.py:213\u001b[0m, in \u001b[0;36mllmAgent.extract_notes\u001b[0;34m(self, encounter_key)\u001b[0m\n\u001b[1;32m    209\u001b[0m documents \u001b[38;5;241m=\u001b[39m loader\u001b[38;5;241m.\u001b[39mload_and_split()\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvectorstores\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FAISS\n\u001b[0;32m--> 213\u001b[0m vector_store \u001b[38;5;241m=\u001b[39m \u001b[43mFAISS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m retriever \u001b[38;5;241m=\u001b[39m vector_store\u001b[38;5;241m.\u001b[39mas_retriever(search_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimilarity\u001b[39m\u001b[38;5;124m\"\u001b[39m, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m List\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/langchain_core/vectorstores/base.py:852\u001b[0m, in \u001b[0;36mVectorStore.from_documents\u001b[0;34m(cls, documents, embedding, **kwargs)\u001b[0m\n\u001b[1;32m    849\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(ids):\n\u001b[1;32m    850\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m ids\n\u001b[0;32m--> 852\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/langchain_community/vectorstores/faiss.py:1041\u001b[0m, in \u001b[0;36mFAISS.from_texts\u001b[0;34m(cls, texts, embedding, metadatas, ids, **kwargs)\u001b[0m\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_texts\u001b[39m(\n\u001b[1;32m   1016\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m   1022\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m FAISS:\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Construct FAISS wrapper from raw documents.\u001b[39;00m\n\u001b[1;32m   1024\u001b[0m \n\u001b[1;32m   1025\u001b[0m \u001b[38;5;124;03m    This is a user friendly interface that:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1039\u001b[0m \u001b[38;5;124;03m            faiss = FAISS.from_texts(texts, embeddings)\u001b[39;00m\n\u001b[1;32m   1040\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1041\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[43membedding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1042\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__from(\n\u001b[1;32m   1043\u001b[0m         texts,\n\u001b[1;32m   1044\u001b[0m         embeddings,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1048\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1049\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/langchain_community/embeddings/huggingface.py:115\u001b[0m, in \u001b[0;36mHuggingFaceEmbeddings.embed_documents\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    113\u001b[0m     sentence_transformers\u001b[38;5;241m.\u001b[39mSentenceTransformer\u001b[38;5;241m.\u001b[39mstop_multi_process_pool(pool)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 115\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow_progress\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_kwargs\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sentence_transformers/SentenceTransformer.py:621\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[0;34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    618\u001b[0m features\u001b[38;5;241m.\u001b[39mupdate(extra_features)\n\u001b[1;32m    620\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 621\u001b[0m     out_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhpu\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    623\u001b[0m         out_features \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(out_features)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sentence_transformers/SentenceTransformer.py:688\u001b[0m, in \u001b[0;36mSentenceTransformer.forward\u001b[0;34m(self, input, **kwargs)\u001b[0m\n\u001b[1;32m    686\u001b[0m     module_kwarg_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_kwargs\u001b[38;5;241m.\u001b[39mget(module_name, [])\n\u001b[1;32m    687\u001b[0m     module_kwargs \u001b[38;5;241m=\u001b[39m {key: value \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m module_kwarg_keys}\n\u001b[0;32m--> 688\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodule_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sentence_transformers/models/Transformer.py:350\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, features, **kwargs)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m features:\n\u001b[1;32m    348\u001b[0m     trans_features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 350\u001b[0m output_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mauto_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtrans_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    351\u001b[0m output_tokens \u001b[38;5;241m=\u001b[39m output_states[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    353\u001b[0m features\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_tokens, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m: features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]})\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:1142\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[1;32m   1140\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m-> 1142\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1143\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1151\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1153\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1154\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1155\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:695\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    684\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    685\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    686\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    692\u001b[0m         output_attentions,\n\u001b[1;32m    693\u001b[0m     )\n\u001b[1;32m    694\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 695\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    706\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:585\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    574\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    575\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    582\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    583\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    584\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 585\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    592\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    594\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:515\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    507\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    513\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    514\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m--> 515\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    524\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[1;32m    525\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:440\u001b[0m, in \u001b[0;36mBertSdpaSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;66;03m# We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\u001b[39;00m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;66;03m# in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\u001b[39;00m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;66;03m# The tgt_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create\u001b[39;00m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;66;03m# a causal mask in case tgt_len == 1.\u001b[39;00m\n\u001b[1;32m    436\u001b[0m is_causal \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    437\u001b[0m     \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_decoder \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_cross_attention \u001b[38;5;129;01mand\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m tgt_len \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    438\u001b[0m )\n\u001b[0;32m--> 440\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout_prob\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    446\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    449\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    450\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mreshape(bsz, tgt_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_head_size)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "results_df = pd.DataFrame(columns=['key', 'rec', 'response'])\n",
    "\n",
    "for key in data['key']:\n",
    "    start = time.time()\n",
    "#try:\n",
    "    print('get_diagnosis')    \n",
    "    diagnosis_dict = llm_agent.extract_diagnosis(encounter_key=key)\n",
    "    print('get_encounter')\n",
    "    encounter_dict = llm_agent.extract_encounter_info(encounter_key=key)\n",
    "    print('get_notes')\n",
    "    notes_dict = llm_agent.extract_notes(encounter_key=key)\n",
    "\n",
    "    results_dict = {\n",
    "        \"diagnosis_dict\": diagnosis_dict,\n",
    "        \"encounter_dict\": encounter_dict,\n",
    "        # Is the reasoning in the json or sepearte?\n",
    "        # Should the reasoning be included in any of them or just the diangosis with the reasoning seperate?\n",
    "        \"notes_dict\": notes_dict,\n",
    "    }\n",
    "    print(results_dict['notes_dict'])\n",
    "    # # #   master formatter step   # # #\n",
    "    # merge the diagnosis booleans (just use OR logic for now)\n",
    "    # make a final \"reasoning\" behind the recommendation\n",
    "    final_dict = merge_results(results_dict=results_dict)\n",
    "\n",
    "    # feed the three reasonings to LLM to get a single summary\n",
    "    final_reasoning = llm_agent.summarize_reasonings(results_dict=results_dict)\n",
    "\n",
    "    # # #   get recommendation from PPI algorithm   # # #\n",
    "    recommendation_str = ppi_deprescribe(patient_diagnosis=final_dict)\n",
    "    response = recommendation_str\n",
    "#except Exception as e:\n",
    "    # new_row = pd.DataFrame([{'key': key, 'rec': \"None\", 'response': \"Bug\"}])\n",
    "    # results_df = pd.concat([results_df, new_row], ignore_index=True)\n",
    "    # print(f\"Bug for {key}\")\n",
    "    # print(e)\n",
    "    # continue\n",
    "    \n",
    "    if response is not None:\n",
    "        new_row = pd.DataFrame([{'key': key, 'rec': response, 'response': final_reasoning}])\n",
    "        results_df = pd.concat([results_df, new_row], ignore_index=True)\n",
    "    else: \n",
    "        new_row = pd.DataFrame([{'key': key, 'rec': \"None\", 'response': \"Bug\"}])\n",
    "        results_df = pd.concat([results_df, new_row], ignore_index=True)\n",
    "    print(f\"{key} took {time.time() - start} seconds to process.\")\n",
    "\n",
    "results_df.to_csv(data_path + '/model_results.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7647\n",
      "Precision (Macro Avg): 0.5093\n",
      "Recall (Macro Avg): 0.6667\n",
      "F1 Score (Macro Avg): 0.5774\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    continue       0.78      1.00      0.88         7\n",
      " deprescribe       0.75      1.00      0.86         6\n",
      "        stop       0.00      0.00      0.00         4\n",
      "\n",
      "    accuracy                           0.76        17\n",
      "   macro avg       0.51      0.67      0.58        17\n",
      "weighted avg       0.58      0.76      0.66        17\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load data\n",
    "ground_truth = pd.read_csv(data_path + '/LabeledResponses.csv')\n",
    "#test = pd.read_csv(data_path + '/model_results.csv')\n",
    "\n",
    "ground_truth = ground_truth.merge(results_df, on='key', how='inner')\n",
    "#test_clean = test.dropna(subset=['rec'])\n",
    "#ground_truth = ground_truth[ground_truth['key'].isin(test_clean['key'])]\n",
    "# Remove rows where 'rec' is None\n",
    "ground_truth = ground_truth[ground_truth['rec'] != 'None']\n",
    "\n",
    "\n",
    "y_true = ground_truth['recommendation']\n",
    "y_pred = ground_truth['rec']\n",
    "class_labels = ['continue', 'deprescribe', 'stop']\n",
    "\n",
    "# Evaluate the model\n",
    "metrics = evaluate_multiclass_classification(y_true, y_pred, class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
